{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liangli217/LLM_learning/blob/main/LLM_for_genomics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9q9qEfw67nG"
      },
      "outputs": [],
      "source": [
        "# DOWNLOAD Mistral-DNA GIT REPO\n",
        "!git clone https://github.com/raphaelmourad/Mistral-DNA.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jET-yGaZ75Xs"
      },
      "outputs": [],
      "source": [
        "# WHEN USING GOOGLE COLLAB\n",
        "!pip install datasets==3.0.1\n",
        "!pip install flash-attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgVbG__p8xzu"
      },
      "outputs": [],
      "source": [
        "# CHECK GPU\n",
        "# We can see how many VRAM is used and how much the GPU is used.\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K03KnxnH-soh"
      },
      "outputs": [],
      "source": [
        "# IMPORT LIBRARIES\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments\n",
        "from transformers import AutoModelForCausalLM, AutoConfig\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXMVmoHiA0FC"
      },
      "outputs": [],
      "source": [
        "#Â SET DIRECTORY\n",
        "os.chdir(\"Mistral-DNA/\")\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ki6qDaBDBO4z"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl5f5SX7-pGu",
        "outputId": "8ee2bd47-312c-4c6c-911e-8fef98c88bc2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MixtralForCausalLM(\n",
              "  (model): MixtralModel(\n",
              "    (embed_tokens): Embedding(4096, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0-7): 8 x MixtralDecoderLayer(\n",
              "        (self_attn): MixtralAttention(\n",
              "          (q_proj): Linear(in_features=256, out_features=256, bias=False)\n",
              "          (k_proj): Linear(in_features=256, out_features=256, bias=False)\n",
              "          (v_proj): Linear(in_features=256, out_features=256, bias=False)\n",
              "          (o_proj): Linear(in_features=256, out_features=256, bias=False)\n",
              "        )\n",
              "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
              "          (gate): Linear(in_features=256, out_features=64, bias=False)\n",
              "          (experts): ModuleList(\n",
              "            (0-63): 64 x MixtralBlockSparseTop2MLP(\n",
              "              (w1): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (w2): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (w3): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (input_layernorm): MixtralRMSNorm((256,), eps=1e-05)\n",
              "        (post_attention_layernorm): MixtralRMSNorm((256,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): MixtralRMSNorm((256,), eps=1e-05)\n",
              "    (rotary_emb): MixtralRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=256, out_features=4096, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# CHOOSE THE LLM ARCHITECTURE\n",
        "# To do during class:\n",
        "# - look at the original archicture of Mixtral-8x7B-v0.1, discuss the model\n",
        "# - change the model architecture by adding or removing transformer blocks, hidden states, number of attention heads, and number of experts\n",
        "# - test BERT model architecture?\n",
        "# NB: flash attention 2 does not work with T4 GPU\n",
        "config = AutoConfig.from_pretrained(\"data/models/Mixtral-8x7B-v0.1\") # Mixture of expert\n",
        "#model = AutoModelForCausalLM.from_config(config,attn_implementation=\"flash_attention_2\")\n",
        "model = AutoModelForCausalLM.from_config(config,attn_implementation=\"eager\")\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USwcaKJOFUlF"
      },
      "outputs": [],
      "source": [
        "# LOAD BPE LETTER TOKENIZER\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
        "tokenizer.padding_side  = 'left'\n",
        "print(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iPahs5IGgxc",
        "outputId": "2e166174-bd6f-46c5-bc29-0cdbb85ad5d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[   1, 2061,  281,  485,    6,    2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ],
      "source": [
        "# DNA encoding\n",
        "encoidng = tokenizer(\"ATTGTGGGTCCCCC\", padding = \"longest\", truncation = True, return_tensors = \"pt\")\n",
        "print(encoidng)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIQNDU6eEp8u",
        "outputId": "b31384c5-d876-4be7-d3de-472d500e3e0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model size: 105.0M parameters\n"
          ]
        }
      ],
      "source": [
        "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model size: {pytorch_total_params/1000**2:.1f}M parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTsGchFNFC4z"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer= tokenizer, mlm = False)\n",
        "dataset_text = load_dataset(\"csv\", data_files=\"data/genome_sequences/hg38/sequences_hg38_200b_verysmall.csv.gz\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH4Spiq-MnFs"
      },
      "outputs": [],
      "source": [
        "tokenized_data = tokenizer(dataset_text['train']['text'], padding=\"longest\", truncation=True, return_tensors=\"pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnGZdqnqNe9N",
        "outputId": "707f1d63-9db2-44b0-aec7-4c808e1a8fee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text'],\n",
              "    num_rows: 99999\n",
              "})"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_text['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7u6-AG4DOThc",
        "outputId": "787aa27a-fac2-448b-faef-7e20959dda22"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 99999\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Syy503vHK5KM"
      },
      "outputs": [],
      "source": [
        "# TOKENIZE DATA\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "dataset = dataset_text.map(tokenize_function, batched=True)\n",
        "# print(dataset[\"train\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MJ9KP-ZMcnQ"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.8 * len(dataset[\"train\"]))\n",
        "test_size = len(dataset[\"train\"]) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(dataset['train'], [train_size, test_size])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFOVWO64dPCc"
      },
      "source": [
        "FP16 vs BF16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGoO3Bk0VtOy",
        "outputId": "47db80b4-e1a3-46ed-b4e2-fcbc35b541e8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=True,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=IntervalStrategy.EPOCH,\n",
            "eval_use_gather_object=False,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=50,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0005,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./logs,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=loss,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=10,\n",
            "optim=OptimizerNames.ADAMW_TORCH,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=./results/models,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=32,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./results/models,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=SaveStrategy.EPOCH,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tp_size=0,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.01,\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Parameters for Pretraining\n",
        "batchsize = 32\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = './results/models',\n",
        "    eval_strategy ='epoch',\n",
        "    save_strategy = 'epoch',\n",
        "    num_train_epochs = 10,\n",
        "    per_device_train_batch_size = batchsize,\n",
        "    per_device_eval_batch_size = batchsize,\n",
        "    learning_rate = 5e-4,\n",
        "    weight_decay = 0.01,\n",
        "    logging_dir = './logs',\n",
        "    load_best_model_at_end = True,\n",
        "    bf16 = True,\n",
        "    gradient_accumulation_steps = 50,\n",
        ")\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "print(training_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Drh9vQWOZPbz"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z4mVT2D3Yq7y"
      },
      "outputs": [],
      "source": [
        "# Pretrain Model\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args=training_args,\n",
        "    data_collator = data_collator,\n",
        "    train_dataset = train_set,\n",
        "    eval_dataset = val_set,\n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "print('Starting a trainer...')\n",
        "# Start training\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPkqQVbw/w+xzwqhZwU8qLV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}