{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNtHXIM7HUJwH+jplYQuLA7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liangli217/LLM_learning/blob/main/LLM_for_genomics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9q9qEfw67nG"
      },
      "outputs": [],
      "source": [
        "# DOWNLOAD Mistral-DNA GIT REPO\n",
        "!git clone https://github.com/raphaelmourad/Mistral-DNA.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WHEN USING GOOGLE COLLAB\n",
        "!pip install datasets==3.0.1\n",
        "!pip install flash-attn"
      ],
      "metadata": {
        "id": "jET-yGaZ75Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHECK GPU\n",
        "# We can see how many VRAM is used and how much the GPU is used.\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "lgVbG__p8xzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORT LIBRARIES\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments\n",
        "from transformers import AutoModelForCausalLM, AutoConfig\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "K03KnxnH-soh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Â SET DIRECTORY\n",
        "os.chdir(\"Mistral-DNA/\")\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "id": "ZXMVmoHiA0FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "Ki6qDaBDBO4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHOOSE THE LLM ARCHITECTURE\n",
        "# To do during class:\n",
        "# - look at the original archicture of Mixtral-8x7B-v0.1, discuss the model\n",
        "# - change the model architecture by adding or removing transformer blocks, hidden states, number of attention heads, and number of experts\n",
        "# - test BERT model architecture?\n",
        "# NB: flash attention 2 does not work with T4 GPU\n",
        "config = AutoConfig.from_pretrained(\"data/models/Mixtral-8x7B-v0.1\") # Mixture of expert\n",
        "#model = AutoModelForCausalLM.from_config(config,attn_implementation=\"flash_attention_2\")\n",
        "model = AutoModelForCausalLM.from_config(config,attn_implementation=\"eager\")\n",
        "model"
      ],
      "metadata": {
        "id": "zl5f5SX7-pGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD BPE LETTER TOKENIZER\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
        "tokenizer.padding_side  = 'left'\n",
        "print(tokenizer)"
      ],
      "metadata": {
        "id": "USwcaKJOFUlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DNA encoding\n",
        "encoidng = tokenizer(\"ATTGTGGGTCCCCC\", padding = \"longest\", truncation = True, return_tensors = \"pt\")\n",
        "print(encoidng)"
      ],
      "metadata": {
        "id": "2iPahs5IGgxc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}