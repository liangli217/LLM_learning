{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liangli217/LLM_learning/blob/main/LLM_for_genomics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9q9qEfw67nG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e52fa117-9044-4980-c2c5-d82d0cfb0921"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Mistral-DNA'...\n",
            "remote: Enumerating objects: 331, done.\u001b[K\n",
            "remote: Counting objects: 100% (96/96), done.\u001b[K\n",
            "remote: Compressing objects: 100% (90/90), done.\u001b[K\n",
            "remote: Total 331 (delta 27), reused 30 (delta 5), pack-reused 235 (from 1)\u001b[K\n",
            "Receiving objects: 100% (331/331), 112.69 MiB | 19.66 MiB/s, done.\n",
            "Resolving deltas: 100% (100/100), done.\n"
          ]
        }
      ],
      "source": [
        "# DOWNLOAD Mistral-DNA GIT REPO\n",
        "!git clone https://github.com/raphaelmourad/Mistral-DNA.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jET-yGaZ75Xs"
      },
      "outputs": [],
      "source": [
        "# WHEN USING GOOGLE COLLAB\n",
        "!pip install datasets==3.0.1\n",
        "!pip install flash-attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgVbG__p8xzu"
      },
      "outputs": [],
      "source": [
        "# CHECK GPU\n",
        "# We can see how many VRAM is used and how much the GPU is used.\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K03KnxnH-soh"
      },
      "outputs": [],
      "source": [
        "# IMPORT LIBRARIES\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments\n",
        "from transformers import AutoModelForCausalLM, AutoConfig\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXMVmoHiA0FC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "776ba5e4-0f72-4b02-a85d-6a8e56097493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Mistral-DNA\n"
          ]
        }
      ],
      "source": [
        "#Â SET DIRECTORY\n",
        "os.chdir(\"Mistral-DNA/\")\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Ki6qDaBDBO4z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0df75a61-d37e-41ce-c306-5a9d094fc24e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data  logs  README.md  results\tscriptPython  scriptR\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl5f5SX7-pGu",
        "outputId": "b0d84ad8-5b9d-4179-90e4-24b593b3e681"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MixtralForCausalLM(\n",
              "  (model): MixtralModel(\n",
              "    (embed_tokens): Embedding(4096, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0-7): 8 x MixtralDecoderLayer(\n",
              "        (self_attn): MixtralAttention(\n",
              "          (q_proj): Linear(in_features=256, out_features=256, bias=False)\n",
              "          (k_proj): Linear(in_features=256, out_features=256, bias=False)\n",
              "          (v_proj): Linear(in_features=256, out_features=256, bias=False)\n",
              "          (o_proj): Linear(in_features=256, out_features=256, bias=False)\n",
              "        )\n",
              "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
              "          (gate): Linear(in_features=256, out_features=64, bias=False)\n",
              "          (experts): ModuleList(\n",
              "            (0-63): 64 x MixtralBlockSparseTop2MLP(\n",
              "              (w1): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (w2): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (w3): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (input_layernorm): MixtralRMSNorm((256,), eps=1e-05)\n",
              "        (post_attention_layernorm): MixtralRMSNorm((256,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): MixtralRMSNorm((256,), eps=1e-05)\n",
              "    (rotary_emb): MixtralRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=256, out_features=4096, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# CHOOSE THE LLM ARCHITECTURE\n",
        "# To do during class:\n",
        "# - look at the original archicture of Mixtral-8x7B-v0.1, discuss the model\n",
        "# - change the model architecture by adding or removing transformer blocks, hidden states, number of attention heads, and number of experts\n",
        "# - test BERT model architecture?\n",
        "# NB: flash attention 2 does not work with T4 GPU\n",
        "config = AutoConfig.from_pretrained(\"data/models/Mixtral-8x7B-v0.1\") # Mixture of expert\n",
        "#model = AutoModelForCausalLM.from_config(config,attn_implementation=\"flash_attention_2\")\n",
        "model = AutoModelForCausalLM.from_config(config,attn_implementation=\"eager\")\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USwcaKJOFUlF"
      },
      "outputs": [],
      "source": [
        "# LOAD BPE LETTER TOKENIZER\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
        "tokenizer.padding_side  = 'left'\n",
        "print(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iPahs5IGgxc",
        "outputId": "b5f422ea-10d7-430e-d102-da07720471b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[   1, 2061,  281,  485,    6,    2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ],
      "source": [
        "# DNA encoding\n",
        "encoidng = tokenizer(\"ATTGTGGGTCCCCC\", padding = \"longest\", truncation = True, return_tensors = \"pt\")\n",
        "print(encoidng)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIQNDU6eEp8u",
        "outputId": "3b0c60fd-e957-400c-ffcf-21fcdbc690c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size: 105.0M parameters\n"
          ]
        }
      ],
      "source": [
        "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model size: {pytorch_total_params/1000**2:.1f}M parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTsGchFNFC4z"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer= tokenizer, mlm = False)\n",
        "\n",
        "# The file contains 100,000 non-overlapping DNA sequences of 200 bases.\n",
        "\n",
        "dataset_text = load_dataset(\"csv\", data_files=\"data/genome_sequences/hg38/sequences_hg38_200b_verysmall.csv.gz\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH4Spiq-MnFs"
      },
      "outputs": [],
      "source": [
        "tokenized_data = tokenizer(dataset_text['train']['text'], padding=\"longest\", truncation=True, return_tensors=\"pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnGZdqnqNe9N",
        "outputId": "3ee2d511-07ed-4358-d988-8a31f5baa091"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text'],\n",
              "    num_rows: 99999\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "dataset_text['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7u6-AG4DOThc",
        "outputId": "9b4e8c4f-9716-4d0b-ded8-b8b31bda5a9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 99999\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "dataset_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Syy503vHK5KM"
      },
      "outputs": [],
      "source": [
        "# TOKENIZE DATA\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "dataset = dataset_text.map(tokenize_function, batched=True)\n",
        "# print(dataset[\"train\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MJ9KP-ZMcnQ"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.8 * len(dataset[\"train\"]))\n",
        "test_size = len(dataset[\"train\"]) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(dataset['train'], [train_size, test_size])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFOVWO64dPCc"
      },
      "source": [
        "FP16 vs BF16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGoO3Bk0VtOy",
        "outputId": "c50d61e8-cb3a-4afa-ee63-5c124dc8aa6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=True,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=IntervalStrategy.EPOCH,\n",
            "eval_use_gather_object=False,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=50,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0005,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./logs,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=loss,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5,\n",
            "optim=OptimizerNames.ADAMW_TORCH,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=./results/models,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=32,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./results/models,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=SaveStrategy.EPOCH,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tp_size=0,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.01,\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Parameters for Pretraining\n",
        "batchsize = 32\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = './results/models',\n",
        "    eval_strategy ='epoch',\n",
        "    save_strategy = 'epoch',\n",
        "    num_train_epochs = 5,\n",
        "    per_device_train_batch_size = batchsize,\n",
        "    per_device_eval_batch_size = batchsize,\n",
        "    learning_rate = 5e-4,\n",
        "    weight_decay = 0.01,\n",
        "    logging_dir = './logs',\n",
        "    load_best_model_at_end = True,\n",
        "    bf16 = True,\n",
        "    gradient_accumulation_steps = 50,\n",
        ")\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "print(training_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Drh9vQWOZPbz"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "Z4mVT2D3Yq7y",
        "outputId": "0f808669-11ee-4214-c244-bf7fabb6256b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting a trainer...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 1:57:54, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.718481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.649852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.579378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.516723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.496838</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=250, training_loss=6.67015625, metrics={'train_runtime': 7104.908, 'train_samples_per_second': 56.298, 'train_steps_per_second': 0.035, 'total_flos': 1.3184106103312896e+16, 'train_loss': 6.67015625, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Pretrain Model\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args=training_args,\n",
        "    data_collator = data_collator,\n",
        "    train_dataset = train_set,\n",
        "    eval_dataset = val_set,\n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "print('Starting a trainer...')\n",
        "# Start training\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvBqWKlRpDem",
        "outputId": "3008f2dc-4ff5-4092-d730-4e22126c51af"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MixtralForCausalLM(\n",
              "  (model): MixtralModel(\n",
              "    (embed_tokens): Embedding(4096, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0-7): 8 x MixtralDecoderLayer(\n",
              "        (self_attn): MixtralAttention(\n",
              "          (q_proj): Linear(in_features=256, out_features=256, bias=False)\n",
              "          (k_proj): Linear(in_features=256, out_features=256, bias=False)\n",
              "          (v_proj): Linear(in_features=256, out_features=256, bias=False)\n",
              "          (o_proj): Linear(in_features=256, out_features=256, bias=False)\n",
              "        )\n",
              "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
              "          (gate): Linear(in_features=256, out_features=64, bias=False)\n",
              "          (experts): ModuleList(\n",
              "            (0-63): 64 x MixtralBlockSparseTop2MLP(\n",
              "              (w1): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (w2): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (w3): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (input_layernorm): MixtralRMSNorm((256,), eps=1e-05)\n",
              "        (post_attention_layernorm): MixtralRMSNorm((256,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): MixtralRMSNorm((256,), eps=1e-05)\n",
              "    (rotary_emb): MixtralRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=256, out_features=4096, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"simple_genomics_model_first_attempt\")"
      ],
      "metadata": {
        "id": "_GjNlQGIrhmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPUTE SEQUENCE EMBEDDING\n",
        "dna = \"ACGTAGCATCGGATCTATCTATCGACACTTGGTTATCGATCTACGAGCATCTCGTTAGC\"\n",
        "inputs = tokenizer(dna, return_tensors = 'pt')[\"input_ids\"]\n",
        "inputs = inputs.to(model.device)  # Assuming model is on the GPU\n",
        "\n",
        "hidden_states = model(inputs)[0] # [1, sequence_length, 4096]\n",
        "print(hidden_states)\n",
        "\n",
        "# embedding with mean pooling\n",
        "embedding_mean = torch.mean(hidden_states[0], dim=0)\n",
        "print(embedding_mean.shape) # expect to be 4096\n",
        "print(embedding_mean)\n",
        "\n",
        "# embedding with max pooling\n",
        "embedding_max = torch.max(hidden_states[0], dim=0)[0]\n",
        "print(embedding_max.shape) # expect to be 4096\n",
        "print(embedding_max)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-5cMuxCvCEE",
        "outputId": "cc08ba43-3bf1-4f0d-841e-4068970b8b76"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-2.4219,  1.7734,  1.4297,  ..., -2.2812, -2.4219, -1.9141],\n",
            "         [-3.6719, -0.1455, -1.3516,  ..., -3.0000, -2.5781, -2.3594],\n",
            "         [-3.5156, -0.2988, -1.4453,  ..., -3.0312, -2.4844, -2.3281],\n",
            "         ...,\n",
            "         [-3.5000,  0.6680,  0.6250,  ..., -2.8438, -2.5156, -2.3906],\n",
            "         [-2.5156, -0.3066,  0.7148,  ..., -2.0000, -1.6328, -1.3984],\n",
            "         [-1.2891,  9.6875,  2.2812,  ..., -1.4688, -1.3750, -1.0312]]],\n",
            "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "torch.Size([4096])\n",
            "tensor([-3.2027,  0.8552,  0.1634,  ..., -2.7399, -2.4026, -2.1227],\n",
            "       device='cuda:0', grad_fn=<MeanBackward1>)\n",
            "torch.Size([4096])\n",
            "tensor([-1.2891,  9.6875,  2.2812,  ..., -1.4688, -1.3750, -1.0312],\n",
            "       device='cuda:0', grad_fn=<MaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_mean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5-k11_iv4-H",
        "outputId": "dcee6c60-8b8c-4fa2-88f7-4663254b8931"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-3.2027,  0.8552,  0.1634,  ..., -2.7399, -2.4026, -2.1227],\n",
              "       device='cuda:0', grad_fn=<MeanBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_max"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvWusg51wLa_",
        "outputId": "facfaaa9-62a1-4dad-c085-06bfe8f6e707"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.2891,  9.6875,  2.2812,  ..., -1.4688, -1.3750, -1.0312],\n",
              "       device='cuda:0', grad_fn=<MaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dna_wt= \"ATTAAAGAAAATATCATCTTTGGTGTTTCCTAT\""
      ],
      "metadata": {
        "id": "MABQlhVXw08q"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dna_mut=\"ATTAAAGAAAATATCATTGGTGTTTCCTAT\""
      ],
      "metadata": {
        "id": "cpu9cb5Fw_U2"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPUTE THE EMBEDDING OF BOTH DNA SEQUENCES\n",
        "inputs_seqs = tokenizer([dna_wt,dna_mut], return_tensors = 'pt', padding=True)[\"input_ids\"]\n",
        "inputs_seqs = inputs_seqs.to(model.device)  # Assuming model is on the GPU\n",
        "\n",
        "hidden_states_seqs = model(inputs_seqs)[0].detach() # [1, sequence_length, 768]\n",
        "\n",
        "embedding_max = torch.max(hidden_states_seqs, dim=1)[0]\n",
        "print(embedding_max)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WmPr0ZxxF06",
        "outputId": "79d072a5-81ed-4fc4-ef8e-31879d45149b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.2891,  9.7500,  2.0312,  ..., -1.4141, -1.3594, -1.0156],\n",
            "        [-0.0884, 10.0000,  2.0000,  ..., -0.3047, -0.3477, -0.3398]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPUTE MUCOVISCIDOSE MUTATION EFFECT (GENE CFTR)\n",
        "distL2=torch.norm(embedding_max[0]-embedding_max[1])\n",
        "print(distL2) # Mutation effect (unnormalized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOm3RKnrxPq8",
        "outputId": "4b4b0742-da9c-4afc-8621-76542cf5cbfd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(58.1931, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPARE WITH A MUTATION THAT DOES NOT IMPACT THE CFTR PROTEIN SEQUENCE\n",
        "dna_wt= \"ATTAAAGAAAATATCATCTTTGGTGTTTCCTAT\"\n",
        "dna_mut=\"ATAAAAGAAAATATCATCTTTGGTGTTTCCTAT\"\n",
        "\n",
        "inputs_seqs = tokenizer([dna_wt,dna_mut], return_tensors = 'pt', padding=True)[\"input_ids\"]\n",
        "inputs_seqs = inputs_seqs.to(model.device)  # Assuming model is on the GPU\n",
        "hidden_states_seqs = model(inputs_seqs)[0].detach() # [1, sequence_length, 768]\n",
        "\n",
        "embedding_max = torch.max(hidden_states_seqs, dim=1)[0]\n",
        "print(embedding_max)\n",
        "\n",
        "distL2=torch.norm(embedding_max[0]-embedding_max[1])\n",
        "print(distL2) # Mutation effect (unnormalized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eT3_K-RxYJD",
        "outputId": "64aaf1f1-fd7f-42cc-d015-b07d25187d09"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.2891,  9.7500,  2.0312,  ..., -1.4141, -1.3594, -1.0156],\n",
            "        [-1.2969,  9.7500,  2.0312,  ..., -1.4219, -1.3672, -1.0156]],\n",
            "       device='cuda:0')\n",
            "tensor(3.2837, device='cuda:0')\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNU801+s/Ml99nUqUJLOzdb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}